{"cells":[{"cell_type":"markdown","metadata":{"id":"GYBQH1O7Y21S"},"source":["# Simple Classifier / Logistic Regression\n","\n","After having worked with the Dataloading part last week, we want to start this week to take a more detailed look into how the training process looks like. So far, our tools are limited and we must restrict ourselves to a simplified model. But nevertheless, this gives us the opportunity to look at the different parts of the training process in more detail and builds up a good base when we turn to more complicated model architectures in the next exercises. \n","\n","This notebook will demonstrate a simple logistic regression model predicting whether a house is ```low-priced``` or ```expensive```. The data that we will use here is the HousingPrice dataset. Feeding some features in our classifier, the output should then be a score that determines in which category the considered house is.\n","\n","![classifierTeaser](images/classifierTeaser.png)"]},{"cell_type":"markdown","metadata":{"id":"xJL5yZqyY21W"},"source":["Before we start, let us first import some libraries and code that we will need along the way. "]},{"cell_type":"markdown","metadata":{"id":"FpjxsB1yY21X"},"source":["## (Optional) Mount folder in Colab\n","\n","Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srJF3pdnY21Y","executionInfo":{"status":"ok","timestamp":1653636383025,"user_tz":-120,"elapsed":3426,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"ec287beb-e134-4aff-8cf8-2d8b80bb52dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","['1_simple_classifier.ipynb', 'exercise_code', 'housing_data_preprocessing(optional).ipynb', 'images']\n"]}],"source":["# Use the following lines if you want to use Google Colab\n","# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n","# NOTE: terminate all other colab sessions that use GPU!\n","# NOTE 2: Make sure the correct exercise folder (e.g exercise_09) is given.\n","\n","# \"\"\"\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_04/exercise_04'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))\n","# \"\"\""]},{"cell_type":"code","execution_count":9,"metadata":{"pycharm":{"name":"#%%\n"},"id":"jaQQx6CjY21Z","executionInfo":{"status":"ok","timestamp":1653636389693,"user_tz":-120,"elapsed":2467,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}}},"outputs":[],"source":["from exercise_code.data.csv_dataset import CSVDataset\n","from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n","from exercise_code.data.dataloader import DataLoader\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import seaborn as sns\n","\n","\n","pd.options.mode.chained_assignment = None  # default='warn'\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"dfza_KIHY21a"},"source":["## 0. Dataloading and Data Preprocessing\n","\n","Let us load the data that we want to use for our training. The method `get_housing_data()` is providing you with a training, validation and test set that is ready to use.\n","\n","For more information about how to prepare the data and what the final data look like, you can have a look at the notebook `housing_data_preprocessing(optional).ipynb `. We reduced our data and the remaining houses in our dataset are now either labeled with ```1``` and hence categorized as ```expensive```, or they are labeled with ```0``` and hence categorized as ```low-priced```.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":543},"id":"WHxDOwapY21a","executionInfo":{"status":"ok","timestamp":1653636692895,"user_tz":-120,"elapsed":4003,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"9ef49917-52d4-413c-a5b9-adc4a9fc5657"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://i2dl.dvl.in.tum.de/downloads/housing_train.zip to /content/gdrive/MyDrive/i2dl/exercise_04/datasets/housing/housing_train.zip\n"]},{"output_type":"stream","name":"stderr","text":["98304it [00:00, 143367.59it/s]                           \n","/content/gdrive/MyDrive/i2dl/exercise_04/exercise_04/exercise_code/networks/utils.py:69: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n","  mn, mx, mean = df.min(), df.max(), df.mean()\n"]},{"output_type":"stream","name":"stdout","text":["You successfully loaded your data! \n","\n","train data shape: (533, 1)\n","train targets shape: (533, 1)\n","val data shape: (167, 1)\n","val targets shape: (167, 1)\n","test data shape: (177, 1)\n","test targets shape: (177, 1) \n","\n","The original dataset looks as follows:\n"]},{"output_type":"execute_result","data":{"text/plain":["      Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n","529  530          20       RL          NaN    32668   Pave   NaN      IR1   \n","491  492          50       RL         79.0     9490   Pave   NaN      Reg   \n","459  460          50       RL          NaN     7015   Pave   NaN      IR1   \n","279  280          60       RL         83.0    10005   Pave   NaN      Reg   \n","655  656         160       RM         21.0     1680   Pave   NaN      Reg   \n","\n","    LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n","529         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n","491         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n","459         Bnk    AllPub  ...        0    NaN    NaN         NaN       0   \n","279         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n","655         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n","\n","    MoSold YrSold  SaleType  SaleCondition  SalePrice  \n","529      3   2007        WD         Alloca     200624  \n","491      8   2006        WD         Normal     133000  \n","459      7   2009        WD         Normal     110000  \n","279      3   2008        WD         Normal     192000  \n","655      3   2010        WD         Family      88000  \n","\n","[5 rows x 81 columns]"],"text/html":["\n","  <div id=\"df-d295a070-1bb8-4332-a0ac-70f65ee1388d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>MSSubClass</th>\n","      <th>MSZoning</th>\n","      <th>LotFrontage</th>\n","      <th>LotArea</th>\n","      <th>Street</th>\n","      <th>Alley</th>\n","      <th>LotShape</th>\n","      <th>LandContour</th>\n","      <th>Utilities</th>\n","      <th>...</th>\n","      <th>PoolArea</th>\n","      <th>PoolQC</th>\n","      <th>Fence</th>\n","      <th>MiscFeature</th>\n","      <th>MiscVal</th>\n","      <th>MoSold</th>\n","      <th>YrSold</th>\n","      <th>SaleType</th>\n","      <th>SaleCondition</th>\n","      <th>SalePrice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>529</th>\n","      <td>530</td>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>NaN</td>\n","      <td>32668</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2007</td>\n","      <td>WD</td>\n","      <td>Alloca</td>\n","      <td>200624</td>\n","    </tr>\n","    <tr>\n","      <th>491</th>\n","      <td>492</td>\n","      <td>50</td>\n","      <td>RL</td>\n","      <td>79.0</td>\n","      <td>9490</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>MnPrv</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>133000</td>\n","    </tr>\n","    <tr>\n","      <th>459</th>\n","      <td>460</td>\n","      <td>50</td>\n","      <td>RL</td>\n","      <td>NaN</td>\n","      <td>7015</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Bnk</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>2009</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>110000</td>\n","    </tr>\n","    <tr>\n","      <th>279</th>\n","      <td>280</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>83.0</td>\n","      <td>10005</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>192000</td>\n","    </tr>\n","    <tr>\n","      <th>655</th>\n","      <td>656</td>\n","      <td>160</td>\n","      <td>RM</td>\n","      <td>21.0</td>\n","      <td>1680</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2010</td>\n","      <td>WD</td>\n","      <td>Family</td>\n","      <td>88000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 81 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d295a070-1bb8-4332-a0ac-70f65ee1388d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d295a070-1bb8-4332-a0ac-70f65ee1388d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d295a070-1bb8-4332-a0ac-70f65ee1388d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}],"source":["from exercise_code.networks.utils import *\n","\n","X_train, y_train, X_val, y_val, X_test, y_test, train_dataset = get_housing_data()\n","\n","print(\"train data shape:\", X_train.shape)\n","print(\"train targets shape:\", y_train.shape)\n","print(\"val data shape:\", X_val.shape)\n","print(\"val targets shape:\", y_val.shape)\n","print(\"test data shape:\", X_test.shape)\n","print(\"test targets shape:\", y_test.shape, '\\n')\n","\n","print('The original dataset looks as follows:')\n","train_dataset.df.head()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"a39iY7lSY21b"},"source":["The data is now ready and can be used to train our classifier model."]},{"cell_type":"markdown","metadata":{"id":"p0GLR4L-Y21b"},"source":["## 1. Set up a Classifier Model\n","\n","Let $\\mathbf{X} \\in \\mathbb{R}^{N\\times (D+1)}$ be our data with $N$ samples and $D$ feature dimensions. With our classifier model, we want to predict binary labels $\\mathbf{\\hat{y}} \\in \\mathbb{R}^{N\\times 1}$. Our classifier model should be of the form\n","\n","$$ \\mathbf{\\hat{y}}  = \\sigma \\left( \\mathbf{X} \\cdot \\mathbf{w} \\right), $$ \n","\n","$ $ where $\\mathbf{w}\\in \\mathbb{R}^{(D+1) \\times 1}$ is the weight matrix of our model.\n","\n","The **sigmoid function** $\\sigma: \\mathbb{R} \\to [0, 1]$, defined by \n","\n","$$ \\sigma(t) = \\frac{1}{1+e^{-t}}, $$\n","\n","is used to squash the outputs of the linear layer into the interval $[0, 1]$. Remember that the sigmoid function is a real-valued function. When applying it on a vector, the sigmoid is operating component-wise.\n","\n","The output of the sigmoid function can be seen as the probability that our sample is indicating a house that can be categorized as ```expensive```. As the probability gets closer to 1, our model is more confident that the input sample is in the class ```expensive```.\n","\n","<img src=\"https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" width=\"800\">"]},{"cell_type":"markdown","metadata":{"id":"GhYdyURpY21c"},"source":["<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p>Take a look at the implementation of the <code>Classifier</code> class in <code>exercise_code/networks/classifier.py</code>. To create a <code>Classifier</code> object, you need to define the number of features that our classifier model takes as input.</p>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"70KzW2cCY21d"},"source":["## 2. Loss: Binary Cross Entropy\n","\n","For a binary classification like our task, we use a loss function called Binary Cross-Entropy (BCE).\n","\n","$$BCE(y,\\hat{y}) =- y \\cdot log(\\hat y ) - (1- y) \\cdot log(1-\\hat y) $$\n","\n","where $y\\in\\mathbb{R}$ is the ground truth and $\\hat y\\in\\mathbb{R}$ is the predicted probability of the house being expensive.\n","\n","Since the BCE function is a non-convex function, there is no closed-form solution for the optimal weights vector. In order to find the optimal parameters for our model, we need to use numeric methods such as Gradient Descent. But let us have a look at that later. First, you have to complete your first task:"]},{"cell_type":"markdown","metadata":{"id":"X6JTDpSaY21d"},"source":["<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>In <code>exercise_code/networks/loss.py</code> complete the implementation of the BCE loss function. You need to write the forward and backward pass of BCE as <code>forward()</code> and <code>backward()</code> function. The backward pass of the loss is needed to later optimize your weights of the model. You can test your implementation by the included testing code in the cell below.</p>\n","</div>"]},{"cell_type":"code","execution_count":13,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"dPlzGN1TY21d","executionInfo":{"status":"ok","timestamp":1653638368147,"user_tz":-120,"elapsed":289,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"f1339ee8-ece3-44d3-c0cc-cd5c2c72cad0"},"outputs":[{"output_type":"stream","name":"stdout","text":["BCEForwardTest passed.\n","BCEBackwardTest passed.\n","Congratulations you have passed all the unit tests!!! Tests passed: 2/2\n","(0, 2)\n"]}],"source":["from exercise_code.tests.loss_tests import *\n","from exercise_code.networks.loss import BCE\n","\n","bce_loss = BCE()\n","print (BCETest(bce_loss)())"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"y7LcHDQvY21e"},"source":["## 3. Backpropagation\n","\n","The backpropagation algorithm allows the information from the loss flowing backward through the network in order to compute the gradient of the loss function $L$ w.r.t the weights $w$ of the model. \n","\n","The key idea of backpropagation is decomposing the derivatives by applying the chain rule to the loss function.\n","\n","$$ \\frac{\\partial L(w)}{\\partial w} = \\frac{\\partial L(w)}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial w}$$\n","\n","You have already completed the `forward()` and `backward()` pass of the loss function, which can be used to compute the derivative  $\\frac{\\partial L(w)}{\\partial \\hat y}$. In order to compute the second term $\\frac{\\partial \\hat y}{\\partial w}$, we need to implement a similar `forward()` and `backward()` method in our `Classifier` class.\n","\n","### Backward Pass\n","\n","The backward pass consists of computing the derivative $\\frac{\\partial \\hat y}{\\partial w}$. Again, we can decompose this derivative by the chain rule: For $s = X \\cdot w$ we obtain\n","\n","$$\\frac{\\partial \\hat y}{\\partial w} = \\frac{\\partial \\sigma(s)}{\\partial w} = \\frac{\\partial \\sigma(s)}{\\partial s} \\cdot \\frac{\\partial s}{\\partial w}$$\n","\n","\n","**Hint:** Taking track of the dimensions in higher-dimensional settings can make the task a little bit complicated. Make sure you understand the operations here. If you have difficulties, first try to understand the forward and backward pass if the input is only one sample consisting of $D+1$ features. Then our data matrix has dimension $X \\in \\mathbb{R}^{1 \\times (D+1)}$. After you understood this situation, you can go back to the setting where our data matrix has dimension $X \\in \\mathbb{R}^{N \\times (D+1)}$ and consists of $N$ samples each having $D+1$ features."]},{"cell_type":"markdown","metadata":{"id":"umzlRwMfY21e"},"source":["<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>Implement the <code>forward()</code> and <code>backward()</code> pass as well as the <code>sigmoid()</code> function in the <code>Classifier</code> class in <code>exercise_code/networks/classifier.py</code>. Check your implementation using the following testing code.</p>\n","</div>"]},{"cell_type":"code","source":["np.matmul([1,2,3],[[1],[2],[3]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNzeozxAmTPJ","executionInfo":{"status":"ok","timestamp":1653639584455,"user_tz":-120,"elapsed":7,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"cddcb88e-870e-404a-9881-6cb683b77d5b"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([14])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","execution_count":62,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"hEx5mdrLY21e","executionInfo":{"status":"ok","timestamp":1653643498199,"user_tz":-120,"elapsed":225,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"4150c001-0c5f-4068-b661-9898fb672433"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sigmoid_Of_Zero passed.\n","Sigmoid_Of_Zero_Array passed.\n","Sigmoid_Of_100 passed.\n","Sigmoid_Of_Array_of_100 passed.\n","Method sigmoid() correctly implemented. Tests passed: 4/4\n","ClassifierForwardTest passed.\n","Method forward() correctly implemented. Tests passed: 1/1\n","ClassifierBackwardTest passed.\n","Method backward() correctly implemented. Tests passed: 1/1\n","Congratulations you have passed all the unit tests!!! Tests passed: 6/6\n","Score: 100/100\n"]},{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{},"execution_count":62}],"source":["from exercise_code.networks.classifier import Classifier\n","from exercise_code.tests.classifier_test import *\n","test_classifier(Classifier(num_features=2))"]},{"cell_type":"markdown","metadata":{"id":"ZopkGk3sY21f"},"source":["## 4. Optimizer and Gradient Descent\n","\n","Previously, we have successfully dealt with the loss function, which is a method of measuring how well our model fits the given data. The idea of the training process is to adjust iteratively the weights of our model in order to minimize the loss function. \n","\n","And this is where the optimizer comes in. In each training step, the optimizer updates the weights of the model w.r.t. the output of the loss function, thereby linking the loss function and model parameters together. The goal is to obtain a model which is accurately predicting the class for a new sample.\n","\n","\n","Any discussion about optimizers needs to begin with the most popular one, and it's called Gradient Descent. This algorithm is used across all types of Machine Learning (and other math problems) to optimize. It's fast, robust, and flexible. Here's how it works:\n","\n","\n","0. Initialize the weights with random values.\n","1. Calculate loss with the current weights and the loss function.\n","2. Calculate the gradient of the loss function w.r.t. the weights.\n","3. Update weights with the corresponding gradient.\n","4. Iteratively perform Step 1 to 3 until converges.\n","\n","The name of the optimizer already hints at the required concept: We use gradients which are very useful for minimizing a function. The gradient of the loss function w.r.t to the weights $w$ of our model tells us how to change our weights $w$ in order to minimize our loss function. \n","\n","The weights are updated each step as follows:\n","$$ w^{(n+1)} = w^{(n)} - \\alpha \\cdot \\frac {dL}{dw}, $$\n","where $ \\frac {dL}{dw}$ is the gradient of your loss function w.r.t. the weights $w$ and $\\alpha$ is the learning rate which is a predefined positive scalar determining the size of the step."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"zSpz_ljmY21f"},"source":["<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>In our model, we will use gradient descent to update the weights. Take a look at the <code>Optimizer</code> class in the file <code>networks/optimizer.py</code>. Your task is now to implement the gradient descent step in the <code>step()</code> method. You can test your implementation by the following testing code.</p>\n","</div>"]},{"cell_type":"code","execution_count":57,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"dqckTk0qY21f","executionInfo":{"status":"ok","timestamp":1653642364888,"user_tz":-120,"elapsed":231,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"24870038-bc1b-4b5e-ec40-bca962da8051"},"outputs":[{"output_type":"stream","name":"stdout","text":["OptimizerStepTest passed.\n","Congratulations you have passed all the unit tests!!! Tests passed: 1/1\n","Score: 100/100\n"]},{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{},"execution_count":57}],"source":["from exercise_code.networks.optimizer import Optimizer\n","from exercise_code.networks.classifier import Classifier\n","from exercise_code.tests.optimizer_test import *\n","TestClassifier=Classifier(num_features=2)\n","TestClassifier.initialize_weights()\n","test_optimizer(Optimizer(TestClassifier))"]},{"cell_type":"markdown","metadata":{"id":"vXskcSs6Y21f"},"source":["## 5. Training\n","\n","We have now implemented all the necessary parts of our training process, namely:\n","- **Classifier Model:** We set up a simple classifier model and you implemented the corresponding ```forward()``` and ```backward()``` methods.\n","- **Loss function:** We chose the Binary Cross Entropy Loss for our model to measure the distance between the prediction of our model and the ground-truth labels. You implemented a forward and backward pass for the loss function.\n","- **Optimizer**: We use the Gradient Descent method to update the weights of our model. Here, you implemented the ```step()``` function which performs the update of the weights. \n","\n","<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p>Before we start our training and put all the parts together, let us shortly talk about the weight initialization. In <code>networks/classifier.py</code> you can check the <code>Classifier</code> class. It contains a method called <code>initialize_weights()</code> that randomly initializes the weights of our classifier model. Later in the lecture, we will learn about more efficient methods to initialize the weights. But for now, a random initialization as it happens in the <code>initialize_weights()</code> method is sufficient.</p>\n","</div>\n","\n","Let's start with our classifier model and look at its performance before any training happened. "]},{"cell_type":"code","execution_count":58,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"iu3U-lsxY21g","executionInfo":{"status":"ok","timestamp":1653642484083,"user_tz":-120,"elapsed":306,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"0b83558a-c48b-4ca1-f7d1-21bc85102c4c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f35c1ba2e10>]"]},"metadata":{},"execution_count":58},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS0klEQVR4nO3df4xdZZ3H8fe301sZXKRoRwNtpaAFLYIiI5BgXMzqUpu1IKC0G7LLhpX4A3YTTbMYCSK6QW3iaiIbF43xx0YQ0TQ11m12FWOWWJap5UcKW1JqtT82MCJl4zLAUL77x72tt3funXtmejtz+/B+JU3uec5zzvO997nz6Zlzzp0bmYkk6eg3Z7YLkCT1hoEuSYUw0CWpEAa6JBXCQJekQsydrYEXLFiQS5Ysma3hJemotHnz5t9l5lC7dbMW6EuWLGFkZGS2hpeko1JE/KbTOk+5SFIhDHRJKoSBLkmFMNAlqRAGuiQVoutdLhHxDeAvgCcy801t1gfwZWAF8AxwVWb+qteFqm7dlj18+kdbeeqZ8YNt8wdr3LTyDL4/8lvueez3HbcN4JjaHMbGX5yBSvvfMQPBs/sP74/TzQGOP7Z2yHy0MxDB/pY/hPea4+Yx+ofnebHRPFibwy2XngXA2o3b2LtvjJPmD7LmotO55OyFrNuyp217O819jx+sEQH7nhnnpPmDvPMNQ9z936Ps3TfGMbU5PPfCi7yY9RrPP/UEdj45VmmMbuN02nYqz6M0R/q5R7e/thgR7wD+AHy7Q6CvAK6jHujnAV/OzPO6DTw8PJzetjg167bsYc1dDzB+mCGk/lYbiEPmeLA2wGXnLOQHm/cwNr7/kPZbLj1zQiCs27KHT/zwoUP6TlenMaqM027bdttMNkZJevXcI2JzZg63W9f1lEtm/gLofNgHF1MP+8zMTcD8iDixcnWqbO3GbYb5S0DrHI+N7+f2e3dNCM6x8f2s3bhtwvZrN27rSZhPNkaVcdpt226bycYoyUw8916cQ18I7Gpa3t1omyAiromIkYgYGR0d7cHQLy17943NdgmaJa2naw5o957o9fuk0/6qjNPa53D2dbSbiec+oxdFM/O2zBzOzOGhobafXNUkTpo/ONslaJYMRLRtb/ee6PX7pNP+qozT2udw9nW0m4nn3otA3wMsblpe1GhTj6256HRqA+1/sFWO1jkerA2w+rzFDNYGJrSvuej0Cduvuej0CX2nq9MYVcZpt227bSYboyQz8dx7Eejrgb+KuvOBpzPzf3qwX7W45OyFrL38zZxwbO2Q9vmDNb50xVu44HWvnHT7oH4nheqO6cF/jnNgwny00+4I+zXHzWNOU/NgbQ5fuuItrL38zSycP0gAC+cPcsulZ/LZS87klkvPnNDe7mLaJWcvPKTv/MEaJxxbO7jdlee/9uC6wdqcgzUMRHDB615ZaYwq47TbtnWbbmOUZCaee5W7XG4HLgQWAI8DnwJqAJn51cZti18BllO/bfFvMrPr7Sve5SJJUzfZXS5d70PPzNVd1ifw0WnWJknqEX//lqRCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEJUCPSKWR8S2iNgeEde3Wf/aiLg7IrZExIMRsaL3pUqSJtM10CNiALgVeA+wDFgdEctaut0A3JmZZwOrgH/udaGSpMlVOUI/F9iemTsy83ngDuDilj4JvKLx+Hhgb+9KlCRVUSXQFwK7mpZ3N9qa3QRcGRG7gQ3Ade12FBHXRMRIRIyMjo5Oo1xJUie9uii6GvhmZi4CVgDfiYgJ+87M2zJzODOHh4aGejS0JAmqBfoeYHHT8qJGW7OrgTsBMvOXwDHAgl4UKEmqpkqg3wcsjYhTImIe9Yue61v6/Bb4M4CIeCP1QPeciiTNoK6BnpkvANcCG4FHqN/NsjUibo6IlY1uHwc+GBEPALcDV2VmHqmiJUkTza3SKTM3UL/Y2dx2Y9Pjh4ELeluaJGkq/KSoJBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKkSlQI+I5RGxLSK2R8T1Hfp8ICIejoitEfHd3pYpSepmbrcOETEA3Aq8G9gN3BcR6zPz4aY+S4FPABdk5lMR8eojVbAkqb0qR+jnAtszc0dmPg/cAVzc0ueDwK2Z+RRAZj7R2zIlSd1UCfSFwK6m5d2NtmanAadFxD0RsSkilrfbUURcExEjETEyOjo6vYolSW316qLoXGApcCGwGvhaRMxv7ZSZt2XmcGYODw0N9WhoSRJUC/Q9wOKm5UWNtma7gfWZOZ6ZvwYepR7wkqQZUiXQ7wOWRsQpETEPWAWsb+mzjvrRORGxgPopmB09rFOS1EXXQM/MF4BrgY3AI8Cdmbk1Im6OiJWNbhuBJyPiYeBuYE1mPnmkipYkTRSZOSsDDw8P58jIyKyMLUlHq4jYnJnD7db5SVFJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCdP2S6L6zeze8/vXw3HOzXYkkTd28ebBzJ5x4Ys93ffQdoe/caZhLOno9/zw8/vgR2fXRd4T+9rfDLP0Nd0nqZ0ffEbokqS0DXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVolKgR8TyiNgWEdsj4vpJ+l0WERkRw70rUZJURddAj4gB4FbgPcAyYHVELGvT7zjg74F7e12kJKm7Kkfo5wLbM3NHZj4P3AFc3KbfZ4DPA8/2sD5JUkVVAn0hsKtpeXej7aCIeCuwODN/PNmOIuKaiBiJiJHR0dEpFytJ6uywL4pGxBzgi8DHu/XNzNsyczgzh4eGhg53aElSkyqBvgdY3LS8qNF2wHHAm4CfR8RO4HxgvRdGJWlmVQn0+4ClEXFKRMwDVgHrD6zMzKczc0FmLsnMJcAmYGVmjhyRiiVJbXUN9Mx8AbgW2Ag8AtyZmVsj4uaIWHmkC5QkVTO3SqfM3ABsaGm7sUPfCw+/LEnSVPlJUUkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklSISoEeEcsjYltEbI+I69us/1hEPBwRD0bETyPi5N6XKkmaTNdAj4gB4FbgPcAyYHVELGvptgUYzsyzgLuAL/S6UEnS5KocoZ8LbM/MHZn5PHAHcHFzh8y8OzOfaSxuAhb1tkxJUjdVAn0hsKtpeXejrZOrgZ+0WxER10TESESMjI6OVq9SktRVTy+KRsSVwDCwtt36zLwtM4czc3hoaKiXQ0vSS97cCn32AIublhc12g4REe8CPgn8aWY+15vyJElVVTlCvw9YGhGnRMQ8YBWwvrlDRJwN/AuwMjOf6H2ZkqRuugZ6Zr4AXAtsBB4B7szMrRFxc0SsbHRbC/wJ8P2IuD8i1nfYnSTpCKlyyoXM3ABsaGm7senxu3pclyRpivykqCQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhZhbpVNELAe+DAwAX8/Mz7WsfxnwbeAc4Engiszc2dtSYd2WPazduI29+8Y4af4gay46nUvOXth2/fGDNSJg3zPjzD+2RiY8PTZ+yHYH+u/ZN3bo8wWy18UfZV4+b4C3LD6eTTueYn8e/quxsGW+2s0lwE3rt7JvbByAE46t8an3nnHIXHWa+3ZuWPcQt9+7i/2ZDERw/qknsPPJsQljfvpHW3nqmfGD280frHHTyjO67l/qN5FdflgjYgB4FHg3sBu4D1idmQ839fkIcFZmfigiVgHvy8wrJtvv8PBwjoyMVC503ZY9fOKHDzE2vv9g22BtgFsuPfPgD3zr+k4GawNcds5CfrB5T6X+6o0D8wVMmKvaQLB/f/Jiyza1geCKty2eMFfNc9/ODese4l83/XbSemoDwf4Xkxfb/AjU5gRr3/9mQ119JyI2Z+Zwu3VVTrmcC2zPzB2Z+TxwB3BxS5+LgW81Ht8F/FlExHQLbmftxm0TwndsfD9rN27ruL6TsfH93H7vLsN8hh2Yr3ZzNd4mzA+0t5ur5rlv5/Z7d3WtZ3x/+zAHGH8xJ92/1I+qBPpCoPmnY3ejrW2fzHwBeBp4VeuOIuKaiBiJiJHR0dEpFbq35bRIa3un9Z304jSCpm7vvrGezdVk++nF/E61Tmm2zehF0cy8LTOHM3N4aGhoStueNH9w0vZO6zsZ6O0vEKropPmDPZuryfbTi/mdap3SbKsS6HuAxU3LixptbftExFzgeOoXR3tmzUWnM1gbOKRtsDZw8MJWu/WdDNYGWH3e4sr91RsH5qvdXNUGou2bsTYQbeeqee7bWX3e4o7rDhmzQ+7X5sSk+5f6UZW7XO4DlkbEKdSDexXwly191gN/DfwSuBz4WXa72jpFBy5OdbrToXV9lbtchk9+pXe5dHCk73KBiXMJne9yOTBXVe9y+ewl9Yuv3uWil5Kud7kARMQK4EvUb1v8Rmb+Y0TcDIxk5vqIOAb4DnA28HtgVWbumGyfU73LRZI0+V0ule5Dz8wNwIaWthubHj8LvP9wipQkHR4/KSpJhTDQJakQBrokFcJAl6RCVLrL5YgMHDEK/Gaamy8AftfDcnrJ2qbH2qbH2qbnaK7t5Mxs+8nMWQv0wxERI51u25lt1jY91jY91jY9pdbmKRdJKoSBLkmFOFoD/bbZLmAS1jY91jY91jY9RdZ2VJ5DlyRNdLQeoUuSWhjoklSIvg70iFgeEdsiYntEXN9m/csi4nuN9fdGxJI+qu0dEfGriHghIi6fqboq1vaxiHg4Ih6MiJ9GxMl9VNuHIuKhiLg/Iv4zIpb1S21N/S6LiIyIGbvtrcLrdlVEjDZet/sj4m/7pbZGnw803nNbI+K7/VJbRPxT02v2aETs66PaXhsRd0fElsbP6oquO83MvvxH/U/1PgacCswDHgCWtfT5CPDVxuNVwPf6qLYlwFnAt4HL++x1eydwbOPxh/vsdXtF0+OVwL/1S22NfscBvwA2AcP9UhtwFfCVmXqfTbG2pcAW4ITG8qv7pbaW/tdR//PgfVEb9YujH248Xgbs7Lbffj5C74svp55ubZm5MzMfhLbffTzbtd2dmc80FjdR/xaqfqntf5sWX87MfddIlfcbwGeAzwPPzlBdU6ltNlSp7YPArZn5FEBmPtFHtTVbDdw+I5VVqy2BVzQeHw/s7bbTfg70nn059SzVNlumWtvVwE+OaEV/VKm2iPhoRDwGfAH4u36pLSLeCizOzB/PUE0HVJ3Tyxq/mt8VEd2/g683qtR2GnBaRNwTEZsiYnkf1QZA47TjKcDPZqAuqFbbTcCVEbGb+vdRXNdtp/0c6DrCIuJKYBhYO9u1NMvMWzPzdcA/ADfMdj0AETEH+CLw8dmupYMfAUsy8yzg3/njb679YC710y4XUj8K/lpEzJ/ViiZaBdyVmftnu5Amq4FvZuYiYAXwncb7sKN+DvS++HLqw6httlSqLSLeBXwSWJmZz/VTbU3uAC45ohX9UbfajgPeBPw8InYC5wPrZ+jCaNfXLTOfbJrHrwPnzEBdlWqjfvS5PjPHM/PXwKPUA74fajtgFTN3ugWq1XY1cCdAZv4SOIb6H+7qbCYuAEzzosFcYAf1X4MOXDQ4o6XPRzn0ouid/VJbU99vMrMXRau8bmdTvyCztA/ndGnT4/dS/97avqitpf/PmbmLolVetxObHr8P2NRHtS0HvtV4vID6qYZX9UNtjX5vAHbS+KBlH71uPwGuajx+I/Vz6JPWOCPFH8aTXkH9f/PHgE822m6mflQJ9f+xvg9sB/4LOLWPansb9SOT/6P+W8PWPqrtP4DHgfsb/9b3UW1fBrY26rp7slCd6dpa+s5YoFd83W5pvG4PNF63N/RRbUH9dNXDwEPUv0S+L2prLN8EfG6maprC67YMuKcxp/cDf95tn370X5IK0c/n0CVJU2CgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEL8PxQfrUTF2mROAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}],"source":["from exercise_code.networks.classifier import Classifier\n","\n","#initialization\n","model = Classifier(num_features=1)\n","model.initialize_weights()\n","\n","y_out, _ = model(X_train)\n","\n","# plot the prediction\n","plt.scatter(X_train, y_train)\n","plt.plot(X_train, y_out, color='r')"]},{"cell_type":"markdown","metadata":{"id":"6J1e_tAZY21g"},"source":["As you can see the predictions of our model without any training are very bad. Let's see how the performance improves when we start our training, which means that we update our weights by applying the gradient descent method. The following cell combines the forward and backward passes with the gradient update step and performs a training step for our classifier:\n","\n","<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p>Note that the <code>Classifier</code> class is derived from the more general <code>Network</code> class. It is worth having a look at the basis class <code>Network</code> in the file <code>exercise_code/networks/base_networks.py</code>. We will make use of the <code>__call__()</code> method, which computes the forward and backward pass of your classifier. In a similar manner, we use the <code>__call__()</code> function for our Loss function.</p>\n","</div>\n","\n","The following cell performs training with 400 training steps:"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4UaMv5ZXY21g","executionInfo":{"status":"ok","timestamp":1653642611148,"user_tz":-120,"elapsed":575,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"cf5b7cea-8072-49e3-c7a1-c52c1fec723d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch  0 --- Average Loss:  0.6931952940067676\n","Epoch  10 --- Average Loss:  0.6857946386752704\n","Epoch  20 --- Average Loss:  0.678678756983829\n","Epoch  30 --- Average Loss:  0.6717725236134877\n","Epoch  40 --- Average Loss:  0.6650655209160695\n","Epoch  50 --- Average Loss:  0.6585512309597763\n","Epoch  60 --- Average Loss:  0.652223452569879\n","Epoch  70 --- Average Loss:  0.6460761046329287\n","Epoch  80 --- Average Loss:  0.6401032275768868\n","Epoch  90 --- Average Loss:  0.6342989944256607\n","Epoch  100 --- Average Loss:  0.6286577204103649\n","Epoch  110 --- Average Loss:  0.62317387062031\n","Epoch  120 --- Average Loss:  0.6178420658053926\n","Epoch  130 --- Average Loss:  0.6126570864902833\n","Epoch  140 --- Average Loss:  0.6076138755716339\n","Epoch  150 --- Average Loss:  0.6027075395725715\n","Epoch  160 --- Average Loss:  0.5979333487262004\n","Epoch  170 --- Average Loss:  0.5932867360528878\n","Epoch  180 --- Average Loss:  0.588763295585977\n","Epoch  190 --- Average Loss:  0.5843587798883735\n","Epoch  200 --- Average Loss:  0.5800690969891076\n","Epoch  210 --- Average Loss:  0.5758903068552539\n","Epoch  220 --- Average Loss:  0.5718186175010208\n","Epoch  230 --- Average Loss:  0.5678503808228264\n","Epoch  240 --- Average Loss:  0.5639820882370234\n","Epoch  250 --- Average Loss:  0.5602103661857649\n","Epoch  260 --- Average Loss:  0.5565319715664128\n","Epoch  270 --- Average Loss:  0.5529437871308838\n","Epoch  280 --- Average Loss:  0.5494428168933815\n","Epoch  290 --- Average Loss:  0.5460261815780151\n","Epoch  300 --- Average Loss:  0.5426911141317895\n","Epoch  310 --- Average Loss:  0.5394349553232765\n","Epoch  320 --- Average Loss:  0.5362551494428555\n","Epoch  330 --- Average Loss:  0.5331492401166684\n","Epoch  340 --- Average Loss:  0.5301148662432714\n","Epoch  350 --- Average Loss:  0.5271497580593237\n","Epoch  360 --- Average Loss:  0.5242517333384513\n","Epoch  370 --- Average Loss:  0.5214186937256059\n","Epoch  380 --- Average Loss:  0.5186486212077477\n","Epoch  390 --- Average Loss:  0.5159395747204676\n"]}],"source":["from exercise_code.networks.optimizer import *\n","from exercise_code.networks.classifier import *\n","# Hyperparameter Setting, we will specify the loss function we use, and implement the optimizer we finished in the last step.\n","num_features = 1\n","\n","# initialization\n","model = Classifier(num_features=num_features)\n","model.initialize_weights()\n","\n","loss_func = BCE() \n","learning_rate = 5e-1\n","loss_history = []\n","opt = Optimizer(model,learning_rate)\n","\n","steps = 400\n","# Full batch Gradient Descent\n","for i in range(steps):\n","    \n","    # Enable your model to store the gradient.\n","    model.train()\n","    \n","    # Compute the output and gradients w.r.t weights of your model for the input dataset.\n","    model_forward, model_backward = model(X_train)\n","    \n","    # Compute the loss and gradients w.r.t output of the model.\n","    loss, loss_grad = loss_func(model_forward, y_train)\n","    \n","    # Use back prop method to get the gradients of loss w.r.t the weights.\n","    grad = loss_grad * model_backward\n","    \n","    # Compute the average gradient over your batch\n","    grad = np.mean(grad, 0, keepdims = True)\n","\n","    # After obtaining the gradients of loss with respect to the weights, we can use optimizer to\n","    # do gradient descent step.\n","    # Take transpose to have the same shape ([D+1,1]) as weights.\n","    opt.step(grad.T)\n","    \n","    # Average over the loss of the entire dataset and store it.\n","    average_loss = np.mean(loss)\n","    loss_history.append(average_loss)\n","    if i%10 == 0:\n","        print(\"Epoch \",i,\"--- Average Loss: \", average_loss)\n"]},{"cell_type":"markdown","metadata":{"id":"nvi9zXedY21h"},"source":["We can see that our average loss is decreasing as expected. Let us visualize the average loss and the prediction after our short training:"]},{"cell_type":"code","execution_count":60,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":559},"id":"84LUYWE4Y21h","executionInfo":{"status":"ok","timestamp":1653642623930,"user_tz":-120,"elapsed":341,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"39465225-50d4-43ec-f3ae-6b4963c904b5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hVVfbw8e9KQgi9JdSEHkCkE3oRcVREFBzpiKAiNsQyI+r8pjjMWOe1iyIooIIgoiIqiCigoJQEpIYWaoKUEHpNW+8f50SvMQk3kJubsj7Pcx/u2afcdQ+QlV3O3qKqGGOMMd4K8HcAxhhjChdLHMYYY3LFEocxxphcscRhjDEmVyxxGGOMyRVLHMYYY3LFEocpkkRkgYiMyOtjCwsRURFpmM2+YSLyTX7HZIoOsec4TEEhIqc9NksDF4A0d/seVZ2R/1FdOhHpAUxX1XA/fLYCkaoadxnXmAYkqOrf8ywwUyQE+TsAYzKoatmM9yKyBxilqt9mPk5EglQ1NT9jM7knIoGqmnbxI01hY01VpsATkR4ikiAij4vIQWCqiFQSkS9FJFFEjrnvwz3OWSoio9z3I0VkuYj8P/fY3SJywyUeW09EfhCRUyLyrYhMEJHpl/CdrnA/97iIbBaRmz329RaRWPcz9ovIX93yUPd7HheRoyKyTERy+j/8JxHZ4R4/QUTE8zu670VEXhaRwyJyUkQ2ikgzERkNDAPGichpEfnCi7inichbIjJfRM4Aj4rIIREJ9DjmzyKyPrf3yxQsljhMYVEdqAzUAUbj/Nud6m7XBs4Bb+RwfgdgGxAKvAC8m/GDNJfHfgisBqoATwHDc/tFRKQE8AXwDVAVeBCYISKN3UPexWmaKwc0Axa75X8BEoAwoBrwNyCntuY+QDugBTAQuD6LY64DugONgArucUmqOgmYAbygqmVV9SYv4gYYCjwNlANeB5Lcz8gwHHg/h5hNIWCJwxQW6cC/VPWCqp5T1SRV/URVz6rqKZwfVlflcP5eVZ3sNp28B9TA+eHr9bEiUhvnB/E/VTVZVZcD8y7hu3QEygLPuddZDHwJDHH3pwBNRaS8qh5T1bUe5TWAOqqaoqrLNOdOyudU9biq7gOWAK2yOCYF54d8E5w+zy2qeuAS4wb4XFV/VNV0VT2Pc/9uAxCRyjjJ68McYjaFgCUOU1gkuj+IABCR0iLytojsFZGTwA9ARc9mkUwOZrxR1bPu27K5PLYmcNSjDCA+l98D9zrxqpruUbYXqOW+vxXoDewVke9FpJNb/j8gDvhGRHaJyBMX+ZyDHu/PksX3dX/4vwFMAA6LyCQRKX+JccMf78d04CYRKYNTm1mWQ2IyhYQlDlNYZP7N+i9AY6CDqpbHaW4ByK75KS8cACqLSGmPsohLuM4vQESm/onawH4AVY1W1b44zUFzgdlu+SlV/Yuq1gduxulDuOYSPv93VPU1VW0LNMVpsnosY1du4s7qHFXdD6wA/ozTTPXB5cZr/M8ShymsyuH0axx3m0D+5esPVNW9QAzwlIgEuzWBmy52noiEeL5w+kjO4nQ8l3CH7d4EzHKvO0xEKqhqCnASp5kOEekjIg3d/pYTOEOV07P8UC+JSDsR6eD2X5wBzntc8xBQ3+PwVdnFfZGPeR8YBzQHPr2ceE3BYInDFFavAKWAI8BK4Ot8+txhQCecTt//Ah/hPG+SnVo4Cc7zFYHzA/cGnPjfBG5X1a3uOcOBPW4T3L3uZwJEAt8Cp3F+i39TVZdc5vcpD0wGjuE0OyXhNImB00nf1B1BNVdVky8Sd3Y+wxnE8FmmZj5TSNkDgMZcBhH5CNiqqj6v8RRmIrITZ6TYH57LMYWP1TiMyQW3aaeBiASISC+gL04/hMmGiNyK0/ex+GLHmsLBnhw3Jneq47TTV8F5puI+Vf3ZvyEVXCKyFKfTfXim0VimELOmKmOMMbliTVXGGGNyxadNVW4b8KtAIPCOqj6Xaf/LwNXuZmmgqqpWdPeNADJm5fyvqr7nlrcFpuGMqJkPPHSRp2cJDQ3VunXr5sVXMsaYYmPNmjVHVDUsc7nPmqrcJ3i3A9fitAVHA0NUNTab4x8EWqvqne64/BggCqdTbQ3QVlWPichqYCzOmPL5wGuquiCnWKKiojQmJiaPvpkxxhQPIrJGVaMyl/uyqao9EKequ9zx37NwRqBkZwgw031/PbBIVY+q6jFgEdBLRGoA5VV1pVvLeB/o57uvYIwxJjNfJo5a/H7emgR+P6fNr0SkDlCP34brZXduLfe9N9ccLSIxIhKTmJh4SV/AGGPMHxWUzvHBwJy8XPRFVSepapSqRoWF/aGJzhhjzCXyZef4fn4/AVw4v58MzdNg4IFM5/bIdO5Stzw8U3l21zTGFFEpKSkkJCRw/vz5ix9sLiokJITw8HBKlCjh1fG+TBzRQKSI1MP54T4YZ5GX3xGRJkAlnLl3MiwEnhGRSu72dcCTqnrUXaWsI07n+O04i8UYY4qRhIQEypUrR926dcl+PS7jDVUlKSmJhIQE6tWr59U5PmuqcteEHoOTBLYAs1V1s4iM91xuEiehzPIcUquqR4H/4CSfaGC8WwZwP/AOzroEO4EcR1QZY4qe8+fPU6VKFUsaeUBEqFKlSq5qbz59jkNV5+MMmfUs+2em7aeyOXcKMCWL8hic5TSNMcWYJY28k9t7WVA6xwuk77Yc4uOYS1ngzRhjii5LHNlQVWas2sfjn2xgwUZb6dIY85ukpCRatWpFq1atqF69OrVq1fp1Ozk5OcdzY2JiGDt2bK4+r27duhw5cuRyQs5TNjtuNkSEN4a25rZ3VvHQrHWUDQmiW6QN6zXGQJUqVVi3bh0ATz31FGXLluWvf/3rr/tTU1MJCsr6x2tUVBRRUX94GLtQsRpHDkoHBzF1ZHvqh5Xhng/WsHbfMX+HZIwpoEaOHMm9995Lhw4dGDduHKtXr6ZTp060bt2azp07s23bNgCWLl1Knz59ACfp3HnnnfTo0YP69evz2muvef15e/bsoWfPnrRo0YJrrrmGffv2AfDxxx/TrFkzWrZsSffu3QHYvHkz7du3p1WrVrRo0YIdO3Zc1ne1GsdFVChdgvfvas+AiSu4Y2o0H93TkSbVy/s7LGOM699fbCb2l5N5es2mNcvzr5uuzPV5CQkJ/PTTTwQGBnLy5EmWLVtGUFAQ3377LX/729/45JNP/nDO1q1bWbJkCadOnaJx48bcd999Xj1P8eCDDzJixAhGjBjBlClTGDt2LHPnzmX8+PEsXLiQWrVqcfz4cQAmTpzIQw89xLBhw0hOTiYt7fKetbYahxeqlgth+l0dCCkRwPB3V7M36Yy/QzLGFEADBgwgMDAQgBMnTjBgwACaNWvGI488wubNm7M858Ybb6RkyZKEhoZStWpVDh065NVnrVixgqFDnUfjhg8fzvLlywHo0qULI0eOZPLkyb8miE6dOvHMM8/w/PPPs3fvXkqVKnVZ39NqHF6KqFya6Xd1YMDbK7jt3VXMubcz1cqH+DssY4q9S6kZ+EqZMmV+ff+Pf/yDq6++ms8++4w9e/bQo0ePLM8pWbLkr+8DAwNJTU29rBgmTpzIqlWr+Oqrr2jbti1r1qxh6NChdOjQga+++orevXvz9ttv07Nnz0v+DKtx5EJktXK8d0d7jp5O5rZ3VpF0+oK/QzLGFFAnTpygVi1nDtZp06bl+fU7d+7MrFmzAJgxYwbdunUDYOfOnXTo0IHx48cTFhZGfHw8u3bton79+owdO5a+ffuyYcOGy/psSxy51DKiIu+MaMe+o2cZ/u5qjp/NeeidMaZ4GjduHE8++SStW7e+7FoEQIsWLQgPDyc8PJxHH32U119/nalTp9KiRQs++OADXn31VQAee+wxmjdvTrNmzejcuTMtW7Zk9uzZNGvWjFatWrFp0yZuv/32y4qlWKw57ouFnL7fnsjd78VwRY1yfDCqA+VDvJsczBhz+bZs2cIVV1zh7zCKlKzuqT8WcirSrmoUxpvD2rD5l5PcMTWa0xcu/zcKY4wpDCxxXIY/Na3G60Nasy7+OHdNi+Zccp4tJ2KMMQWWJY7LdEPzGrw0sCWr9xxl9AcxnE+x5GFMfigOzez5Jbf30hJHHujbqhYv3NqCZTuOcP+MtSSnpvs7JGOKtJCQEJKSkix55IGM9ThCQrx/vMCe48gjA6IiSE5L5/8+28SDM9fyxtA2lAi0vGyML4SHh5OQkEBiYqK/QykSMlYA9JYljjw0rEMdklPT+fcXsTzy0TpeGdSKIEsexuS5EiVKeL1ancl7ljjy2B1d6pGcms6zC7YSGCC8OKClJQ9jTJFiicMH7rmqAanpyv8WbkMVXhpoycMYU3T49KeZiPQSkW0iEiciT2RzzEARiRWRzSLyoVt2tYis83idF5F+7r5pIrLbY18rX36HS/XA1Q0Z16sx89b/wiOz15OaZh3mxpiiwWc1DhEJBCYA1wIJQLSIzFPVWI9jIoEngS6qekxEqgKo6hKglXtMZSAO+Mbj8o+p6hxfxZ5X7u/REEF4/uutALxsNQ9jTBHgy6aq9kCcqu4CEJFZQF8g1uOYu4EJqnoMQFUPZ3Gd/sACVT3rw1h95r4eDRCB5xZsRVWtw9wYU+j58idYLSDeYzvBLfPUCGgkIj+KyEoR6ZXFdQYDMzOVPS0iG0TkZREpmcU5iMhoEYkRkRh/D9m796oGPHlDE77ccICHPlpnzVbGmELN37/6BgGRQA9gCDBZRCpm7BSRGkBzYKHHOU8CTYB2QGXg8awurKqTVDVKVaPCwvy/Vvg9VzXgb72b8NWGAzw0y5KHMabw8mVT1X4gwmM73C3zlACsUtUUYLeIbMdJJNHu/oHAZ+5+AFT1gPv2gohMBf5KITG6ewME4en5WwB4ZXAre0jQGFPo+PKnVjQQKSL1RCQYp8lpXqZj5uLUNhCRUJymq10e+4eQqZnKrYUgIgL0Azb5Inhfubt7ff5+4xV8tfEAD836mRSreRhjChmf1ThUNVVExuA0MwUCU1R1s4iMB2JUdZ677zoRiQXScEZLJQGISF2cGsv3mS49Q0TCAAHWAff66jv4yqhu9QH471dbSE1by+tDW1MyKNDPURljjHdsISc/mvbjbp76IpYejcOYeFtbQkpY8jDGFBy2kFMBNLJLPZ79c3O+357IHVOjOWOLQRljCgFLHH42pH1tXhrYklW7k7h9ympOnk+5+EnGGONHljgKgFtah/PG0Dasjz/OsMmrOHYm2d8hGWNMtixxFBC9m9fg7eFt2XboFEMmr+TI6Qv+DskYY7JkiaMAueaKakwZ0Y49SWcY9PYKDp447++QjDHmDyxxFDBdI0N5/84OHDxxnoFvryDhWKGcossYU4RZ4iiA2terzPRRHTh+NpmBE1ew+8gZf4dkjDG/ssRRQLWuXYmZoztyPjWdARNXEPvLSX+HZIwxgCWOAu3KmhWYfU9HSgQKgyatIGbPUX+HZIwxljgKuoZVy/HxvZ0ILVuS295dxdJtWS1ZYowx+ccSRyEQXqk0H9/bifqhZRn1Xgzz1v/i75CMMcWYJY5CIrRsSWbd05E2tSvx0Kyfmb5yr79DMsYUU5Y4CpHyISV4/672XN24Kn+fu4kJS+IoDpNUGmMKFkschUxIiUDeHt6Wvq1q8r+F23hm/hZLHsaYfOXLFQCNj5QIDODlga2oWKoEk5ft5sS5FJ65pTlBtpqgMSYfWOIopAIChKduvpIKpYN57bsdnDiXwquDW9uaHsYYn7NfUQsxEeHRaxvxzz5NWbj5EMPfXcWJszYtuzHGtyxxFAF3dq3H60Nasz7+BAPe/olfjp/zd0jGmCLMEkcRcVPLmky7ox2/HD/PrW/9xPZDp/wdkjGmiPJp4hCRXiKyTUTiROSJbI4ZKCKxIrJZRD70KE8TkXXua55HeT0RWeVe8yMRCfbldyhMOjcMZfY9nUhLV/q/9ROrd9sUJcaYvOezxCEigcAE4AagKTBERJpmOiYSeBLooqpXAg977D6nqq3c180e5c8DL6tqQ+AYcJevvkNh1LRmeT65rzOh5ZwpSr7edNDfIRljihhf1jjaA3GquktVk4FZQN9Mx9wNTFDVYwCqmuNETCIiQE9gjlv0HtAvT6MuAiIql2bOvZ25smZ57p+xhg/sKXNjTB7yZeKoBcR7bCe4ZZ4aAY1E5EcRWSkivTz2hYhIjFuekRyqAMdVNTWHawIgIqPd82MSExMv/9sUMpXLBPPhqI5c3bgq/5i7iRe/2WYPChpj8oS/O8eDgEigBzAEmCwiFd19dVQ1ChgKvCIiDXJzYVWdpKpRqhoVFhaWlzEXGqWCnafMB0VF8PriOB7/ZAMpaen+DssYU8j5MnHsByI8tsPdMk8JwDxVTVHV3cB2nESCqu53/9wFLAVaA0lARREJyuGaxkNQYADP3dqcsddEMjsmgTunRXPyvD3rYYy5dL5MHNFApDsKKhgYDMzLdMxcnNoGIhKK03S1S0QqiUhJj/IuQKw6bS1LgP7u+SOAz334HYqEjAcFX+jfghU7kxg4cYU962GMuWQ+SxxuP8QYYCGwBZitqptFZLyIZIySWggkiUgsTkJ4TFWTgCuAGBFZ75Y/p6qx7jmPA4+KSBxOn8e7vvoORc3AqAim3dGe/cfO0W/Cj2zaf8LfIRljCiEpDh2mUVFRGhMT4+8wCoxtB09xx9TVHD+XwhtDW9OzSTV/h2SMKYBEZI3b1/w7/u4cN37QuHo55j7QhfphZRj1XowN1zXG5IoljmKqavkQPhrd6dfhus/M30J6etGvfRpjLp8ljmKsTMkgJt0exe2d6jDph1088OFazqek+TssY0wBZ4mjmAsMEP5985X8/cYr+HrzQYZMXkniqQv+DssYU4BZ4jCICKO61eetYW3YcuAk/Sb8yJYDJ/0dljGmgLLEYX7Vq1kNPr6nM6np6dz61k8sij3k75CMMQWQJQ7zO83DKzBvTFcaVi3L6A9iePv7nTbHlTHmdyxxmD+o5o646t28Bs8u2MpjczZwIdU6zY0xjqCLH2KKo1LBgbwxpDUNw8ry6nc72Jt0hom3taVK2ZL+Ds0Y42dW4zDZEhEeubYRrw1pzYaEE/Sd8CPbDtqStMYUd5Y4zEXd3LImH93TiQupTqf5kq05rrdljCniLHEYr7SKqMi8MV2oU6U0d70XzTvLdlmnuTHFlCUO47UaFUrx8b2duK5pdf771Rb+8vF6e9LcmGLIEofJldLBQbw5rA0P/ymST9fuZ+DbtraHMcWNJQ6TawEBwsN/asSk4W3Zefg0N7+xnOg9R/0dljEmn1jiMJfsuiurM/eBLpQLKcGQSSuZbtOzG1MsWOIwlyWymrO2R9fIUP4+dxNPfrrRHhY0poi7aOIQkRdEpLyIlBCR70QkUURuy4/gTOFQoVQJ3h3Rjvt7NGDm6n0MnbyKwyfP+zssY4yPeFPjuE5VTwJ9gD1AQ+Axby4uIr1EZJuIxInIE9kcM1BEYkVks4h86Ja1EpEVbtkGERnkcfw0EdktIuvcVytvYjG+FRggjOvVhDeGtib2l5Pc9MZyft53zN9hGWN8wJvEkTEtyY3Ax6p6wpsLi0ggMAG4AWgKDBGRppmOiQSeBLqo6pXAw+6us8Dtblkv4BURqehx6mOq2sp9rfMmHpM/+rSoySf3daZEYACD3l7J7Oh4f4dkjMlj3iSOL0VkK9AW+E5EwgBv2iHaA3GquktVk4FZQN9Mx9wNTFDVYwCqetj9c7uq7nDf/wIcBsK8+ULG/5rWLM8XY7rSrl4lxn2ygSc/3WDPexhThFw0cajqE0BnIEpVU4Az/DEBZKUW4PnrZoJb5qkR0EhEfhSRlSLSK/NFRKQ9EAzs9Ch+2m3CellEspx1T0RGi0iMiMQkJiZ6Ea7JS5XKBPPeHe2596oGzFwdz4CJK0g4dtbfYRlj8oA3neMDgBRVTRORvwPTgZp59PlBQCTQAxgCTPZskhKRGsAHwB2qmu4WPwk0AdoBlYHHs7qwqk5S1ShVjQoLs8qKPwQFBvDEDU14e3hb9hw5Q5/Xl/P9dkvixhR23jRV/UNVT4lIV+BPwLvAW16ctx+I8NgOd8s8JQDzVDVFVXcD23ESCSJSHvgK+D9VXZlxgqoeUMcFYCpOk5gpwK6/sjrzHuxK9fIhjJy6mle/3UF6us1zZUxh5U3iyGicvhGYpKpf4TQdXUw0ECki9UQkGBgMzMt0zFyc2gYiEorTdLXLPf4z4H1VneN5glsLQUQE6Ads8iIW42f1Qsvw6f2d6deqFi9/u50734vm+Nlkf4dljLkE3iSO/SLyNjAImO/2KXjTN5IKjAEWAluA2aq6WUTGi8jN7mELgSQRiQWW4IyWSgIGAt2BkVkMu50hIhuBjUAo8F+vv63xq9LBQbw0sCX/6deMH+OO0Of15Wza79UgPWNMASIXmxpbRErjDIndqKo73N/4m6vqN/kRYF6IiorSmJgYf4dhPPy87xj3z1hL0plk/tP3Sga1q+3vkIwxmYjIGlWNylzuTc3hLM6IputFZAxQtTAlDVMwta5diS8f7Er7upV5/JONjJuznnPJNmTXmMLAm1FVDwEzgKrua7qIPOjrwEzRV6VsSd67sz1jrm7I7JgE+k34kbjDp/0dljHmIrxpqtoAdFLVM+52GWCFqrbIh/jyhDVVFXzfb0/kkY/WcT4ljadvacYtrcP9HZIxxd4lN1UBwm8jq3DfS14FZgzAVY3CmD+2G81qVeCRj9bz+JwN1nRlTAEVdPFDmAqsEpHP3O1+OM9yGJOnqlcI4cNRHXjl2x1MWBrHuvjjTBjWhoZVy/o7NGOMB286x18C7gCOuq87VPUVXwdmiqegwAD+en1j3rujPUdOX+DmN5bz6doEf4dljPGQbR+HiFTO6URVLTRrhVofR+F06OR5xs78mVW7jzIwKpx/39yMUsGB/g7LmGIjuz6OnJqq1gDKb/0ZGRlG3Pf18zRCYzKpVj6EGaM68Op3O3hjidt0NbQNkdXK+Ts0Y4q1bJuqVLWeqtZ3/8x4n7FtScPki6DAAP5yndN0lXQ6mZveWM7M1fu42GhAY4zv2JrjplDo3iiMBQ91I6pOZZ78dCP3z1hrc10Z4yeWOEyhUbV8CO/f2Z4nb2jCothD3PDqMlbtSvJ3WMYUO5Y4TKESECDcc1UDPr2/MyWDAhgyeSUvfbON1LT0i59sjMkT3kw5UjmLV4n8CM6Y7LQIr8iXY7txS+twXlscx6BJK4k/aisMGpMfvKlxrAUScRZZ2uG+3yMia0WkrS+DMyYnZUsG8eLAlrw6uBXbD56i92vL+GL9L/4Oy5giz5vEsQjoraqhqloFuAH4ErgfeNOXwRnjjb6tajH/oW40rFqWB2f+zGMfr+fMhVR/h2VMkeVN4uioqgszNtwp1Tu5y7mW9FlkxuRCROXSzL6nE2OubsictQnc+Noy1u475u+wjCmSvEkcB0TkcRGp477GAYdEJBCwHklTYJRwpyuZeXdHUtKUARNX8NKi7aRYx7kxecqbxDEUCMdZH3wuUNstC8RZ4tWYAqVj/SoseLgbfVvV5LXvdnDrWz+xM9HW+TAmr1x0PY6iwOaqKr7mbzzA3z7byPmUNP7W+wqGd6yDiK0KYIw3Lnk9DhFpJCKTROQbEVmc8fLyQ3uJyDYRiRORJ7I5ZqCIxIrIZhH50KN8hIjscF8jPMrbishG95qvif0UMDno3bwG3zzcnQ71qvDPzzczYmo0h06e93dYxhRq3qwAuB6YiDPp4a8r66jqmoucF4gzhPdaIAGIBoaoaqzHMZHAbKCnqh4TkaqqetidmTcGiMKZUHEN0NY9ZjUwFlgFzAdeU9UFOcViNQ6jqkxftY+nv4olpEQgz9zSnN7Na/g7LGMKtMtZATBVVd9S1dWquibj5cV57YE4Vd2lqsnALKBvpmPuBiao6jEAVT3sll8PLFLVo+6+RUAvEakBlFfVlepkvPdxFpYyJkciwvCOdfhqbDfqVC7N/TPW8uhH6zh5PsXfoRlT6HiTOL4QkftFpIbn0+NenFcLiPfYTnDLPDUCGonIjyKyUkR6XeTcWu77nK4JgIiMFpEYEYlJTEz0IlxTHDQIK8uc+zrz0DWRfL7+F254ZRk/xh3xd1jGFCreJI4RwGPATzhNRmtwmpHyQhAQCfQAhgCTRaRiXlxYVSepapSqRoWFheXFJU0RUSIwgEeubcSceztRMiiAYe+s4h9zN9lDg8Z4yZulY+tl8fJmPY79QITHdrhb5ikBmKeqKaq6G6dPJDKHc/e773O6pjFeaV27El+N7cZdXesxfdVeer36Ayt22my7xlxMtolDRHq6f/45q5cX144GIkWknogEA4OBeZmOmYtT20BEQnGarnYBC4HrRKSSiFQCrgMWquoB4KSIdHRHU90OfJ6bL2yMp1LBgfyjT1Nm39OJQBGGTF7Jvz7fxNlkq30Yk52clo69ClgM3JTFPgU+zenCqpoqImNwkkAgMEVVN4vIeCBGVefxW4KIxRmx9ZiqJgGIyH9wkg/AeI81zu8HpgGlgAXuy5jL0q5uZRY81J0XFm5l6o97WLItkf/1b0GH+lX8HZoxBY49AGhMJit3JTFuzgbij51lZOe6jLu+CaWCA/0dljH5LrvhuDnVODJOLAncCtT1PF5Vx+dlgMYUFB3rV+Hrh7vx/AKn9rHUrX1E1fVmMKExRZ83o6o+x3n+IhU44/EypsgqHRzEv/s248O7O5CSls6At1fw3y9jOZecdvGTjSnivHlyfJOqNsuneHzCmqrM5Th9IZVn529hxqp91K5cmuf+3JzODUP9HZYxPnc5T47/JCLNfRCTMYVC2ZJBPH1Lc2aN7kiAwNB3VvH4nA2cOGtPnZviyZvE0RVY405WuMGdYHCDrwMzpqBx+j66c89V9ZmzNoE/vfw9X2864O+wjMl33jRV1cmqXFX3+iQiH7CmKpPXNu0/wbg5G4g9cJIbmlXn332vpGq5EH+HZUyeynVTlYiUd9+eyuZlTLHVrFYFPh/Thceub8x3Ww/zpxe/Z3Z0PMVheLsxOTVVZayNkTE31Rryfq4qYwqtEoEBPHB1Q7sLHYMAABhsSURBVBY81I0m1csz7pMN3PbuKvYlnfV3aMb4lD0AaEweSE9XPly9j+cWbCU1PZ1Hr23EnV3qERToTTeiMQXT5Yyqwp0zqr2IdM945X2IxhReAQHCbR3rsOjR7nRtGMoz87dy0xs/8vO+Y/4OzZg8583SsaOAH3Dmlfq3++dTvg3LmMKpRoVSTL49iom3teHYmWT+/NZP/GPuJlswyhQp3tQ4HgLaAXtV9WqgNXDcp1EZU4iJCL2a1WDRo90Z0akuM1bt5ZoXv+eL9b9Y57kpErxJHOdV9Tw481ap6lagsW/DMqbwKxdSgqduvpLPH+hKtfIleXDmz4ycGm2d56bQ8yZxJLir8s0FFonI50CheYbDGH9rHl6Bufd34Z99mhKz5yjXvvw9E5bEkZya7u/QjLkkuRpVJSJXARWAr1U12WdR5TEbVWUKigMnzvHvebF8vfkgjaqV5elbmtPOZt01BdQljaoSkUAR2Zqxrarfq+q8wpQ0jClIalQoxcThbXl3RBRnLqQxYOIKHvt4PUdOX/B3aMZ4LcfEoappwDYRqZ1P8RhTLFxzRTUWPerMe/XZz/vp+f+W8v6KPaSlW+e5Kfi8mavqB5yRVKvxWIdDVW/2bWh5x5qqTEEWd/g0T83bzPK4IzStUZ7/9LuStnWs+cr4X3ZNVd4kjquyKlfV77340F7Aqzhrjr+jqs9l2j8S+B+w3y16Q1XfEZGrgZc9Dm0CDFbVuSIyDWc99BPuvpGqui6nOCxxmIJOVZm/8SD//SqWAyfOc2ubcJ64oQlh5Ur6OzRTjF3y0rFAb1V9PNPFngdyTBwiEghMAK4FEoBoEZmnqrGZDv1IVcd4FqjqEqCVe53KQBzwjcchj6nqHC9iN6ZQEBFubFGDHo3DeGNJHO8s28U3sQf5y7WNuK1jHZu6xBQo3vxrvDaLshu8OK89EKequ9zO9Fk4S9DmVn9ggara4HdT5JUpGcTjvZrw9cPdaRVRkae+iKXP68uJ3nPU36EZ86ucplW/T0Q2Ao3dBZwyXrsBbxZyqgXEe2wnuGWZ3eped46IRGSxfzAwM1PZ0+45L4tIlnV5ERktIjEiEpOYmOhFuMYUHA3CyvL+ne15c1gbTp5LYcDEFTw6ex2HT533d2jGZN/HISIVgErAs8ATHrtOqepFf/0Rkf5AL1Ud5W4PBzp4NkuJSBXgtKpeEJF7gEGq2tNjfw2cJFVTVVM8yg4CwcAkYKeqjs8pFuvjMIXZ2eRU3lgcx+RluygZFMiDPRsysktdSgYF+js0U8Tl+jkOVT2hqntUdYiq7vV4eVtn3g941iDC+a0TPOMzklQ1YwD7O0DbTNcYCHyWkTTccw6o4wIwFadJzJgiq3RwEON6NWHhw91pX68yzy7YyvUv/8C3sYds7ivjF77scYsGIkWknogE4zQ5zfM8wK09ZLgZ2JLpGkPI1EyVcY6ICNAP2JTHcRtTINUPK8uUke2Ydkc7AgOEUe/HcPuU1Ww/ZAtymvzlzaiqS6KqqSIyBmca9kBgiqpuFpHxQIyqzgPGisjNQCpwFBiZcb6I1MWpsWQevTVDRMIAAdYB9/rqOxhTEPVoXJUuDUOZvnIvLy/azg2vLuO2DrV55NpGVCwd7O/wTDFgKwAaU4gdPZPMy4u2M2PVXsqFlODRaxsxrENtG75r8sRlrQBojCmYKpcJ5j/9mjH/oW40q1Wef83bTO/XlrFsh40kNL5jicOYIqBJ9fJMv6sDk4a35XxKOsPfXc2o96LZlXja36GZIsgShzFFhIhw3ZXVWfRod564oQkrdx3lupd/4F+fbyLJZt81ecgShzFFTMmgQO69qgFL/tqDwe0jmL5qHz3+t5S3lu7kfEqav8MzRYAlDmOKqLByJflvv+YsfLg7HepX4fmvt3LNi98z9+f9pNv07eYyWOIwpohrWLUs74yIYubdHalUpgQPf7SOmycsZ8XOJH+HZgopSxzGFBOdGlRh3gNdeXlQS46eTmbI5JWMei+auMP2AKHJHUscxhQjAQHCLa3DWfzXHjzeqwmrdh3l+leW8fe5G235WuM1ewDQmGIs6fQFXvtuB9NX7aNkUACjutXn7m71KBdSwt+hmQLgklcALAoscRiTs12Jp3nxm+18tfEAlcsE88DVDbmtY22bgbeYs8RhicOYi1off5wXFm7lx7gkalUsxaPXNqJf61oEBoi/QzN+YFOOGGMuqmVERWaM6sj0uzpQuUwwf/l4Pb1fXWZTuJvfscRhjPmDrpGhfP5AFyYMbUNyWjqj3o9hwMQVtoStASxxGGOyERAg3NiiBt880p1nbmnOvqNnGTBxBXdNi2brwZP+Ds/4kfVxGGO8ci45jWk/7eGtpXGcupBKv1a1eOiaSOqGlvF3aMZHrHPcEocxeeLE2RTe/D6O937aQ0qa0r9NOA9e05DwSqX9HZrJY5Y4LHEYk6cOnzrPm0t28uGqfSjKoHYRjLk6kuoVQvwdmskjljgscRjjEwdOnOONxXHMjolHRLitQx3u69GAsHIl/R2auUyWOCxxGONT8UfP8vriHXyydj/BgQHc3rkO93RvQOUytg56YeWX5zhEpJeIbBOROBF5Iov9I0UkUUTWua9RHvvSPMrneZTXE5FV7jU/EhH7V2lMARBRuTQv9G/Jt49exfVXVmPSD7vo9vxiXvxmGyfOpfg7PJOHfFbjEJFAYDtwLZAARANDVDXW45iRQJSqjsni/NOqWjaL8tnAp6o6S0QmAutV9a2cYrEahzH5b8ehU7zy7Q6+2niA8iFB3N2tPiO61KW8zYNVaPijxtEeiFPVXaqaDMwC+l7OBUVEgJ7AHLfoPaDfZUVpjPGJyGrlmDCsDV+N7UqH+lV4cdF2uj63mFe+3W41kELOl4mjFhDvsZ3glmV2q4hsEJE5IhLhUR4iIjEislJEMpJDFeC4qqZe5JqIyGj3/JjExMTL/CrGmEt1Zc0KTL49ii8f7ErH+lV45dsddH1uMS99s43jZ5P9HZ65BP5+cvwLoK6qtgAW4dQgMtRxq0hDgVdEpEFuLqyqk1Q1SlWjwsLC8i5iY8wlaVarApNuj2L+2G50axTKa4vj6PLcYl74eitHz1gCKUx8mTj2A541iHC37FeqmqSqGavHvAO09di33/1zF7AUaA0kARVFJCi7axpjCramNcvz5rC2LHy4O1c3qcpb3++k6/OLeXb+FhJP2WJShYEvE0c0EOmOggoGBgPzPA8QkRoemzcDW9zySiJS0n0fCnQBYtXpyV8C9HfPGQF87sPvYIzxkcbVy/HG0DYseqQ71zWtxuRlu+j2wmL+82Ush0+e93d4Jgc+fY5DRHoDrwCBwBRVfVpExgMxqjpPRJ7FSRipwFHgPlXdKiKdgbeBdJzk9oqqvutesz5OR3tl4GfgNo9aS5ZsVJUxBd+uxNNMWLKTuev2ExggDG1fm9Hd61OzYil/h1Zs2QOAljiMKRT2Jp3hzSU7+WRtAiLQr1Ut7u3RgAZhfxidb3zMEoclDmMKlYRjZ5n8wy5mRceTnJbODc2qc3+PhjSrVcHfoRUbljgscRhTKB05fYEpy3fzwYq9nLqQSrfIUB64uiEd6lXGebTL+IolDkscxhRqJ8+nMH3lXqYs382R08m0qV2R+3s05JorqloC8RFLHJY4jCkSzqekMTsmnre/38X+4+doXK0c91/dgBub1yAo0N+PphUtljgscRhTpKSkpfPF+l94c+lO4g6fpnbl0ozuXp/+bcMJKRHo7/CKBEscljiMKZLS05VFWw7x5tKdrI8/TpUywYzoXJfhHetQyaZ0vyyWOCxxGFOkqSordiUx+YddLNmWSEiJAAZGRXBX13rUqWLrol+K7BJHUFYHG2NMYSMidG4QSucGoWw/dIrJP+xi5up9TF+5l17NqnN3t/q0rl3J32EWCVbjMMYUWYdOnmfaT3uYvnIvp86n0r5uZUZ3r0/PJlUJCLCRWBdjTVWWOIwptk5fSOWj6HimLN/N/uPnaBBWhru71adf61rWkZ4DSxyWOIwp9lLT0pm/6SCTftjJpv0nCS1bkpGd6zC0Qx1bGz0LljgscRhjXBkd6ZN+2MXSbYmUDArgz21qcUeXejSqVs7f4RUY1jlujDEuz470HYdOMfWnPXy6NoGZq+Pp2jCUO7vWpUcj6wfJjtU4jDEGOHYmmZnR+3j/p70cPHmeeqFlGNm5Lv3bhlOmZPH8HduaqixxGGO8kJKWzoJNB3l3+W7Wxx+nXEgQg9tFcHunukRULu3v8PKVJQ5LHMaYXFq77xhTlu9mwaaDqCrXNa3OnV3r0a5upWIxsaL1cRhjTC61qV2JNkMr8cvxc3ywci8frtrH15sPcmXN8tzRpR59WtQolsN5rcZhjDFeOpucymc/72fK8t3sTDxD5TLBDGoXwbAOtQmvVPSasbKrcfh0DmIR6SUi20QkTkSeyGL/SBFJFJF17muUW95KRFaIyGYR2SAigzzOmSYiuz3OaeXL72CMMRlKBwcxrEMdvn30Kqbf1YG2dSrx9vc76f7CEka9F8MP2xNJTy/6v4z7rKlKRAKBCcC1QAIQLSLzVDU206EfqeqYTGVngdtVdYeI1ATWiMhCVT3u7n9MVef4KnZjjMmJiNA1MpSukaEkHDvLh6v2MSs6nm+3HKJ+aBlu61iH/lHhlA8p4e9QfcKXNY72QJyq7lLVZGAW0NebE1V1u6rucN//AhwGwnwWqTHGXKLwSqUZ16sJPz3Rk5cGtqR8qRKM/zKWjs98x98+28jWgyf9HWKe82XiqAXEe2wnuGWZ3eo2R80RkYjMO0WkPRAM7PQofto952URKZnVh4vIaBGJEZGYxMTEy/gaxhhzcSElAvlzm3DmPtCFeWO60Lt5DeasSaDXK8sYOHEFX274hZS0dH+HmSd81jkuIv2BXqqa0W8xHOjg2SwlIlWA06p6QUTuAQapak+P/TWApcAIVV3pUXYQJ5lMAnaq6vicYrHOcWOMPxw7k8zsmHimr9pL/NFzVC1XksHtazO4XQQ1K5byd3gXle/PcYhIJ+ApVb3e3X4SQFWfzeb4QOCoqlZwt8vjJI1nsuvPEJEewF9VtU9OsVjiMMb4U1q6snTbYd5fsZcfdiQiwNWNqzK0Q216NK5KYAGd2sQfz3FEA5EiUg/YDwwGhmYKqoaqHnA3bwa2uOXBwGfA+5mTRsY54jx90w/Y5MPvYIwxly0wQLjmimpcc0U14o+eZVb0Pj6KTuC7rTHUqBDCoHYRDGoXQY0KBb8WAj5+jkNEegOvAIHAFFV9WkTGAzGqOk9EnsVJGKnAUeA+Vd0qIrcBU4HNHpcbqarrRGQxTke5AOuAe1X1dE5xWI3DGFPQpKSl823sIT5cvY9lO44QINCzSTWGdahN90ZhBaIWYlOOWOIwxhRQe5POMHN1PHPWxHPkdDK1KpZicLsIBraLoFr5EL/FZYnDEocxpoBLTk3nm9iDzFy9jx/jkpwmriZOX0j3yLB8n+bd5qoyxpgCLjgogD4tatKnRU12HznDrNX7+HhNAt/EHiK8UikGRkXQv22430dkWY3DGGMKsAupaSzcfIhZq/fx084kRKB7ZBiD2kXwpyuqERzku8fxrKnKEocxppDbl3SWj9fEM2dNAgdOnKdymWD6tarFoHYRNK6e90veWuKwxGGMKSLS0pUfdiTycUw8i2IPkZKmtIyoyKCoCG5qWYNyeTRHliUOSxzGmCIo6fQFPvt5P7Nj4tl+6DSlSgTSu3kNBrWLuOwFpyxxWOIwxhRhqsq6+OPMjonni/UHOH0hlXqhZZh4W9tLbsayUVXGGFOEiQita1eide1K/KNPU+ZvPMi89b8QUTnvR2BZ4jDGmCKmdHAQ/duG079tuE+u79MVAI0xxhQ9ljiMMcbkiiUOY4wxuWKJwxhjTK5Y4jDGGJMrljiMMcbkiiUOY4wxuWKJwxhjTK4UiylHRCQR2HuJp4cCR/IwnLxiceVOQY0LCm5sFlfuFMW46qhqWObCYpE4LoeIxGQ1V4u/WVy5U1DjgoIbm8WVO8UpLmuqMsYYkyuWOIwxxuSKJY6Lm+TvALJhceVOQY0LCm5sFlfuFJu4rI/DGGNMrliNwxhjTK5Y4jDGGJMrljhyICK9RGSbiMSJyBN+jmWPiGwUkXUiEuOWVRaRRSKyw/2zUj7EMUVEDovIJo+yLOMQx2vu/dsgIm3yOa6nRGS/e8/WiUhvj31PunFtE5HrfRhXhIgsEZFYEdksIg+55X69ZznE5dd7JiIhIrJaRNa7cf3bLa8nIqvcz/9IRILd8pLudpy7v24+xzVNRHZ73K9Wbnm+/dt3Py9QRH4WkS/dbd/eL1W1VxYvIBDYCdQHgoH1QFM/xrMHCM1U9gLwhPv+CeD5fIijO9AG2HSxOIDewAJAgI7AqnyO6yngr1kc29T9+ywJ1HP/ngN9FFcNoI37vhyw3f18v96zHOLy6z1zv3dZ930JYJV7H2YDg93yicB97vv7gYnu+8HARz66X9nFNQ3on8Xx+fZv3/28R4EPgS/dbZ/eL6txZK89EKequ1Q1GZgF9PVzTJn1Bd5z378H9PP1B6rqD8BRL+PoC7yvjpVARRGpkY9xZacvMEtVL6jqbiAO5+/bF3EdUNW17vtTwBagFn6+ZznElZ18uWfu9z7tbpZwXwr0BOa45ZnvV8Z9nANcIyKSj3FlJ9/+7YtIOHAj8I67Lfj4flniyF4tIN5jO4Gc/2P5mgLfiMgaERntllVT1QPu+4NANf+Elm0cBeEejnGbCqZ4NOX5JS63WaA1zm+rBeaeZYoL/HzP3GaXdcBhYBFO7ea4qqZm8dm/xuXuPwFUyY+4VDXjfj3t3q+XRaRk5riyiDmvvQKMA9Ld7Sr4+H5Z4ig8uqpqG+AG4AER6e65U526p9/HVheUOFxvAQ2AVsAB4EV/BSIiZYFPgIdV9aTnPn/esyzi8vs9U9U0VW0FhOPUaprkdwxZyRyXiDQDnsSJrx1QGXg8P2MSkT7AYVVdk5+fa4kje/uBCI/tcLfML1R1v/vnYeAznP9QhzKqv+6fh/0UXnZx+PUequoh9z97OjCZ35pW8jUuESmB88N5hqp+6hb7/Z5lFVdBuWduLMeBJUAnnKaeoCw++9e43P0VgKR8iquX2+SnqnoBmEr+368uwM0isgenOb0n8Co+vl+WOLIXDUS6oxOCcTqS5vkjEBEpIyLlMt4D1wGb3HhGuIeNAD73R3w5xDEPuN0dYdIROOHRPONzmdqUb8G5ZxlxDXZHmNQDIoHVPopBgHeBLar6kscuv96z7OLy9z0TkTARqei+LwVci9P/sgTo7x6W+X5l3Mf+wGK3BpcfcW31SP6C04/geb98/veoqk+qariq1sX5GbVYVYfh6/uVlz37Re2FMzJiO04b6//5MY76OCNa1gObM2LBaZv8DtgBfAtUzodYZuI0YaTgtJ3elV0cOCNKJrj3byMQlc9xfeB+7gb3P0wNj+P/z41rG3CDD+PqitMMtQFY5756+/ue5RCXX+8Z0AL42f38TcA/Pf4PrMbplP8YKOmWh7jbce7++vkc12L3fm0CpvPbyKt8+7fvEWMPfhtV5dP7ZVOOGGOMyRVrqjLGGJMrljiMMcbkiiUOY4wxuWKJwxhjTK5Y4jDGGJMrljiMyQUR+cn9s66IDM3ja/8tq88ypqCx4bjGXAIR6YEzi2yfXJwTpL/NH5TV/tOqWjYv4jPGl6zGYUwuiEjGDKnPAd3cNRgecSfA+5+IRLsT3t3jHt9DRJaJyDwg1i2b605WuTljwkoReQ4o5V5vhudnuU8f/09ENomzJssgj2svFZE5IrJVRGb4YmZYYzILuvghxpgsPIFHjcNNACdUtZ07Q+qPIvKNe2wboJk605ED3KmqR92pK6JF5BNVfUJExqgziV5mf8aZdLAlEOqe84O7rzVwJfAL8CPO3EXL8/7rGvMbq3EYkzeuw5mbaB3O9ORVcOZzAljtkTQAxorIemAlzoRzkeSsKzBTnckHDwHf48zGmnHtBHUmJVwH1M2Tb2NMDqzGYUzeEOBBVV34u0KnL+RMpu0/AZ1U9ayILMWZP+hSXfB4n4b9nzb5wGocxlyaUzhLrmZYCNznTlWOiDRyZzLOrAJwzE0aTXCWFc2QknF+JsuAQW4/ShjOMrk+mc3XGG/YbyfGXJoNQJrb5DQNZw2EusBat4M6kayX8v0auFdEtuDMMrvSY98kYIOIrFVnauwMn+GsSbEeZ0bbcap60E08xuQ7G45rjDEmV6ypyhhjTK5Y4jDGGJMrljiMMcbkiiUOY4wxuWKJwxhjTK5Y4jDGGJMrljiMMcbkyv8H1CAiqijAFq8AAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Plot the loss history to see how it goes after several steps of gradient descent.\n","plt.plot(loss_history, label = 'Train Loss')\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.legend()\n","plt.show()\n","\n","\n","# forward pass\n","y_out, _ = model(X_train)\n","\n","\n","# plot the prediction\n","plt.scatter(X_train, y_train, label = 'Ground Truth')\n","inds = X_train.argsort(0).flatten()\n","plt.plot(X_train[inds], y_out[inds], color='r', label = 'Prediction')\n","plt.title('Prediction of our trained model')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e4lbt1e1Y21h"},"source":["This looks pretty good already and our model gets better in explaining the underlying relationship of data."]},{"cell_type":"markdown","metadata":{"id":"0GXug48vY21h"},"source":["## 6. Solver\n","\n","Now we want to put everything we have learned so far together in an organized and concise way, that provides easy access to train a network/model in your own script/code. The purpose of a solver is mainly to provide an abstraction for all the gritty details behind training your parameters, such as logging your progress, optimizing your model, and handling your data.\n","\n","This part of the exercise will require you to complete the missing code in the ```Solver``` class and to train your model end to end.\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"XnKkcihBY21h"},"source":["<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>Open the file <code>exercise_code/solver.py</code> and have a look at the <code>Solver</code> class. The <code>_step()</code> function is representing one single training step. So when using the Gradient Descent method, it represents one single update step using the Gradient Descent method. Your task is now to finalize this <code>_step()</code> function. You can test your implementation with the testing code included in the following cell.</p>\n","    <p> <b>Hint</b>: The implementation of the <code>_step()</code> function is very similar to the implementation of a training step as we observed above. You may have a look at that part first. </p>\n","</div>"]},{"cell_type":"code","execution_count":76,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"xpOgrrt5Y21h","executionInfo":{"status":"ok","timestamp":1653644950932,"user_tz":-120,"elapsed":278,"user":{"displayName":"Umaid Bin Zubair","userId":"04715560925550446121"}},"outputId":"18732a1f-6394-44f0-ed85-ab001dc14b95"},"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 1)\n","SolverStepTest passed.\n","Congratulations you have passed all the unit tests!!! Tests passed: 1/1\n","Score: 100/100\n"]},{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{},"execution_count":76}],"source":["from exercise_code.solver import Solver\n","from exercise_code.networks.classifier import Classifier\n","from exercise_code.tests.solver_tests import *\n","weights = np.array([[0.1],[0.1]])\n","TestClassifier = Classifier(num_features=1)\n","TestClassifier.initialize_weights(weights)\n","learning_rate = 5e-1\n","data = {'X_train': X_train, 'y_train': y_train,\n","        'X_val': X_val, 'y_val': y_val}\n","loss = BCE()\n","solver = Solver(TestClassifier,data,loss,learning_rate,verbose=True)\n","\n","test_solver(solver)"]},{"cell_type":"markdown","metadata":{"id":"GtF0gMTaY21i"},"source":["After having successfully implemented the `step()` function in the `Optimizer` class, let us now train our classifier. We train our model with a learning rate $ \\lambda = 0.1$ and with 25000 epochs. Your model should reach an accuracy which is higher than 85%. "]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"1bcVzSWBY21i","outputId":"4231c4dd-6a5c-4a57-82cc-33062be633d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(Epoch 16000 / 25000) train loss: 0.342606; val_loss: 0.355955\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(Epoch 17000 / 25000) train loss: 0.340298; val_loss: 0.353982\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(Epoch 18000 / 25000) train loss: 0.338260; val_loss: 0.352261\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(Epoch 19000 / 25000) train loss: 0.336452; val_loss: 0.350752\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(Epoch 20000 / 25000) train loss: 0.334841; val_loss: 0.349426\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n","(2, 1)\n"]}],"source":["from exercise_code.solver import Solver\n","from exercise_code.networks.utils import test_accuracy\n","from exercise_code.networks.classifier import Classifier\n","# Select the number of features, you want your task to train on.\n","# Feel free to play with the sizes.\n","num_features = 1\n","\n","# initialize model and weights\n","model = Classifier(num_features=num_features)\n","model.initialize_weights()\n","\n","y_out, _ = model(X_test)\n","\n","accuracy = test_accuracy(y_out, y_test)\n","print(\"Accuracy BEFORE training {:.1f}%\".format(accuracy*100))\n","\n","\n","if np.shape(X_test)[1]==1:\n","    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n","    inds = X_test.flatten().argsort(0)\n","    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n","    plt.legend()\n","    plt.show()\n","\n","data = {'X_train': X_train, 'y_train': y_train,\n","        'X_val': X_val, 'y_val': y_val}\n","\n","#We use the BCE loss\n","loss = BCE()\n","\n","# Please use these hyperparmeter as we also use them later in the evaluation\n","learning_rate = 1e-1\n","epochs = 25000\n","\n","# Setup for the actual solver that's going to do the job of training\n","# the model on the given data. set 'verbose=True' to see real time \n","# progress of the training.\n","solver = Solver(model, \n","                data, \n","                loss,\n","                learning_rate, \n","                verbose=True, \n","                print_every = 1000)\n","# Train the model, and look at the results.\n","solver.train(epochs)\n","\n","\n","# Test final performance\n","y_out, _ = model(X_test)\n","\n","accuracy = test_accuracy(y_out, y_test)\n","print(\"Accuracy AFTER training {:.1f}%\".format(accuracy*100))"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"gejTenTBY21i"},"source":["During the training process losses in each epoch are stored in the lists `solver.train_loss_history` and `solver.val_loss_history`. We can use them to plot the training result easily."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"EZWuK1CFY21i"},"outputs":[],"source":["plt.plot(solver.val_loss_history, label = \"Validation Loss\")\n","plt.plot(solver.train_loss_history, label = \"Train Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend() \n","plt.title('Training and Validation Loss')\n","plt.show() \n","\n","\n","if np.shape(X_test)[1]==1:\n","\n","    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n","    inds = X_test.argsort(0).flatten()\n","    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n","    plt.legend()\n","    plt.title('Prediction of your trained model')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"H6zZPsxIY21i"},"source":["## 7. Save your BCE Loss, Classifier and Solver for Submission\n","\n","Your model should be trained now and able to predict whether a house is expensive or not. Hooooooray, you trained your very first model! The model will be saved as a pickle file to `models/simple_classifier.p`."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"HvQqzlzcY21i"},"outputs":[],"source":["from exercise_code.tests import save_pickle\n","\n","save_pickle(\n","    data_dict={\n","        \"BCE_class\": BCE,\n","        \"Classifier_class\": Classifier,\n","        \"Optimizer\": Optimizer,\n","        \"Solver_class\": Solver\n","    },\n","    file_name=\"simple_classifier.p\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"hrnZQ_fUY21j"},"source":["# Submission Instructions\n","\n","Now, that you have completed the necessary parts in the notebook, you can go on and submit your files.\n","\n","1. Go on [our submission page](https://i2dl.dvl.in.tum.de), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum-online and check your mails there. You will get an id which we need in the next step.\n","2. Log into [our submission page](https://i2dl.dvl.in.tum.de) with your account details and upload the zip file.\n","3. Your submission will be evaluated by our system and you will get feedback about the performance of it. You will get an email with your score as well as a message if you have surpassed the threshold.\n","4. Within the working period, you can submit as many solutions as you want to get the best possible score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uz2xlim5Y21j"},"outputs":[],"source":["from exercise_code.submit import submit_exercise\n","\n","submit_exercise('exercise04')"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"HEH6_-JdY21j"},"source":["# Submission Goals\n","\n","For this exercise we only test your implementations which are tested throughout the notebook.  In total we have 10 test cases where you are required to complete 8 of. Here is an overview split among the notebook:\n","\n","- Goal: \n","    - To implement: \n","        1. `exercise_code/networks/loss.py`: `forward()`, `backward()`\n","        2. `exercise_code/networks/classifier.py`: `forward()`, `backward()`, `sigmoid()`\n","        3. `exercise_code/networks/optimizer.py`: `step()`\n","        4. `exercise_code/solver.py`: `_step()`\n","\n","    - Test cases:\n","      1. Does `forward()` of `BCE` return the correct value?\n","      2. Does `backward()` of `BCE` return the correct value?\n","      3. Does `sigmoid()` of `Classifier` return the correct value when `x=0`?\n","      4. Does `sigmoid()` of `Classifier` return the correct value when `x=np.array([0,0,0,0,0])`?\n","      5. Does `sigmoid()` of `Classifier` return the correct value when `x=100`?\n","      6. Does `sigmoid()` of `Classifier` return the correct value when `x=np.asarray([100, 100, 100, 100, 100])`?\n","      7. Does `forward()` of `Classifier` return the correct value?\n","      8. Does `backward()` of `Classifier` return the correct value?\n","      9. Does `Optimizer` update the model parameter correctly?\n","      10. Does `Solver` update the model parameter correctly?\n","\n","\n","- Reachable points [0, 100]: 0 if not implemented, 100 if all tests passed, 10 per passed test\n","- Threshold to clear exercise: 80\n","- Submission start: __May 24, 2022 16:00__\n","- Submission deadline: __May 30, 2022 23:59__\n","- You can make multiple submissions until the deadline. Your __best submission__ will be considered for bonus."]},{"cell_type":"markdown","metadata":{"id":"-7UMtG-3Y21j"},"source":["# [Exercise Review](https://docs.google.com/forms/d/e/1FAIpQLScwZArz6ogLqBEj--ItB6unKcv0u9gWLj8bspeiATrDnFH9hA/viewform)\n","\n","We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://docs.google.com/forms/d/e/1FAIpQLScwZArz6ogLqBEj--ItB6unKcv0u9gWLj8bspeiATrDnFH9hA/viewform) for this exercise so that we can do better next time! :)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"name":"1_simple_classifier.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}